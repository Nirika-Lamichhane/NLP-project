{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5709744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# dataset_path = '/content/training_dataset.csv'\n",
    "dataset_path = '/content/training_dataset (1).csv'\n",
    "df = pd.read_csv(dataset_path, header=None, names=[\"comment\",\"target\",\"aspect\",\"sentiment\"])\n",
    "df['aspect'] = df['aspect'].astype(str).str.lower()\n",
    "df['sentiment'] = df['sentiment'].astype(str).str.lower()\n",
    "# Drop unused column\n",
    "df = df.drop(columns=[\"target\"])\n",
    "\n",
    "print(\"Dataset sample:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc053b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_aspects = [\"policy\", \"governance\", \"service\", \"economy\", \"corruption\"]\n",
    "valid_sentiments = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "df = df[df['aspect'].isin(valid_aspects)]\n",
    "df = df[df['sentiment'].isin(valid_sentiments)]\n",
    "\n",
    "df = df[df['comment'].apply(lambda x: 3 <= len(str(x).split()) <= 11)]\n",
    "\n",
    "print(\"Filtered dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_path = '/content/drive/MyDrive/cc.ne.300.vec.gz'\n",
    "fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_path)\n",
    "\n",
    "print(\"FastText vocab size:\", len(fasttext_model.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_comment(comment, max_len=50):\n",
    "    \"\"\"\n",
    "    Convert a comment into a fixed-length embedding matrix.\n",
    "    - Tokenize into words.\n",
    "    - Map each word to FastText vector (300-dim).\n",
    "    - Pad or truncate to max_len tokens.\n",
    "    \"\"\"\n",
    "    tokens = str(comment).strip().split()\n",
    "    vectors = []\n",
    "    for tok in tokens:\n",
    "        if tok in fasttext_model.key_to_index:\n",
    "            vectors.append(fasttext_model[tok])\n",
    "        else:\n",
    "            vectors.append(np.zeros(fasttext_model.vector_size))\n",
    "    # Pad / truncate\n",
    "    if len(vectors) < max_len:\n",
    "        pad = [np.zeros(fasttext_model.vector_size)] * (max_len - len(vectors))\n",
    "        vectors.extend(pad)\n",
    "    else:\n",
    "        vectors = vectors[:max_len]\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Build embeddings\n",
    "X = np.stack([embed_comment(c) for c in df['comment']])\n",
    "print(\"Embeddings shape:\", X.shape)   # (num_samples, max_len, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45ef8e",
   "metadata": {},
   "source": [
    "## Label Encoding of Output Variables\n",
    "\n",
    "- `LabelEncoder` is a class provided by the **`sklearn.preprocessing`** module.\n",
    "- Two independent `LabelEncoder` objects are created to separately encode **aspect** and **sentiment** labels.\n",
    "- Each encoder learns and stores the unique class labels in the `classes_` attribute.\n",
    "- The categorical labels are transformed into integer values, as machine learning models require numerical inputs.\n",
    "- Label encoding is performed **before one-hot encoding**, since one-hot encoding operates on integer-encoded labels.\n",
    "\n",
    "### Working of `fit_transform()`\n",
    "\n",
    "- `fit_transform()` performs the **fitting** and **transformation** steps in a single operation.\n",
    "\n",
    "#### `fit()`\n",
    "- Identifies all unique class labels in the data\n",
    "- Sorts the labels lexicographically\n",
    "- Stores the sorted labels in `encoder.classes_`\n",
    "- learns so done in the training data\n",
    "- Should not fit() again on validation data **serious bug** \n",
    "\n",
    "1. If fit didnt run then classes_ doesnot exist\n",
    "\n",
    "#### `transform()`\n",
    "- Replaces each categorical label with its corresponding integer index\n",
    "- it doesnot learn anything just replaces the label with integer index only on the validation/test data i.e. already trained ones\n",
    "\n",
    "### Working of 'to_categorical()'\n",
    "- it is from tensorflow.keras.utils\n",
    "- this is **one hot encoding**\n",
    "- it converts integer class labels into probability target vectors that neural network can learn from.\n",
    "\n",
    "#### to_categorical() line of code\n",
    "- this is using the integer labels we just obtained from the fit_transform() and is also calculating the total number of aspects and sentiment labels then applying one hot encoding \n",
    "1. e.g. if y_aspect_int = [2, 0, 1] and num_classes = 4 then\n",
    "\n",
    "y_aspect =\n",
    "[\n",
    " [0, 0, 1, 0],  # governance\n",
    " [1, 0, 0, 0],  # corruption\n",
    " [0, 1, 0, 0]   # economy\n",
    "]\n",
    " this will be the result after the line of code\n",
    "\n",
    "- this is done because the loss function is valid only if y is one hot vector. where y is target i.e. aspect and sentiment are our targets\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03acd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_encoder = LabelEncoder()\n",
    "sentiment_encoder = LabelEncoder()\n",
    "\n",
    "y_aspect_int = aspect_encoder.fit_transform(df['aspect'])\n",
    "y_sentiment_int = sentiment_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "y_aspect = to_categorical(y_aspect_int, num_classes=len(valid_aspects))\n",
    "y_sentiment = to_categorical(y_sentiment_int, num_classes=len(valid_sentiments))\n",
    "\n",
    "print(\"Aspect classes:\", aspect_encoder.classes_)\n",
    "print(\"Sentiment classes:\", sentiment_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fdcd3f",
   "metadata": {},
   "source": [
    "## StratifiedShuffleSplit\n",
    "\n",
    "- this splits the datasets preserving the class distribution of targets.\n",
    "- it is a object not the split itself.\n",
    "- In this project, datasets are imbalanced as some classes of aspects or sentiment has many datas than other, so to maintain the **same proportion** of each class in both train and test sets we used this.\n",
    "\n",
    "### Stratify\n",
    "1. It means to split the dataset so that each subset has the same proportion of the classes as the original datasets.\n",
    "2. Essential in multi class classification.\n",
    "\n",
    "### Random_state=42\n",
    "- Its significance is that we can get the same combination of samples and classes in our training and test splits i.e. fixed randomness.\n",
    "\n",
    "### sss.split()\n",
    "- It doesnot care about the number of classes directly. It uses the class labels and returns the index of the samples in X. where X is comment as target is dropped.\n",
    "- Indexes are row number of X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cbdcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in sss.split(X, y_aspect_int):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_aspect_train, y_aspect_test = y_aspect[train_idx], y_aspect[test_idx]\n",
    "    y_sent_train, y_sent_test = y_sentiment[train_idx], y_sentiment[test_idx]\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(50, 300))\n",
    "\n",
    "# CNN + GlobalMaxPooling\n",
    "conv = Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "\n",
    "# BiLSTM\n",
    "bilstm = Bidirectional(LSTM(128))(input_layer)\n",
    "\n",
    "# Concatenate CNN + BiLSTM features\n",
    "merged = tf.keras.layers.concatenate([pool, bilstm])\n",
    "\n",
    "# Shared dropout\n",
    "drop = Dropout(0.5)(merged)\n",
    "\n",
    "# Task-specific dense layers\n",
    "sentiment_dense = Dense(64, activation='relu')(drop)\n",
    "sentiment_output = Dense(3, activation='softmax', name=\"sentiment\")(sentiment_dense)\n",
    "\n",
    "aspect_dense = Dense(64, activation='relu')(drop)\n",
    "aspect_output = Dense(5, activation='softmax', name=\"aspect\")(aspect_dense)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=input_layer, outputs=[sentiment_output, aspect_output])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\"sentiment\": \"categorical_crossentropy\", \"aspect\": \"categorical_crossentropy\"},\n",
    "    metrics={\"sentiment\": [\"accuracy\"], \"aspect\": [\"accuracy\"]}\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {\"sentiment\": y_sent_train, \"aspect\": y_aspect_train},\n",
    "    validation_data=(X_test, {\"sentiment\": y_sent_test, \"aspect\": y_aspect_test}),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training vs Validation Loss ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_validation_loss.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "# --- Training vs Validation Accuracy (Sentiment) ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(history.history['sentiment_accuracy'], label='Training Sentiment Accuracy')\n",
    "plt.plot(history.history['val_sentiment_accuracy'], label='Validation Sentiment Accuracy')\n",
    "plt.title('Training vs Validation Accuracy (Sentiment)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_validation_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49278677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_probs, aspect_probs = model.predict(X_test)\n",
    "\n",
    "y_sent_pred = np.argmax(sentiment_probs, axis=1)\n",
    "y_asp_pred = np.argmax(aspect_probs, axis=1)\n",
    "\n",
    "y_sent_true = np.argmax(y_sent_test, axis=1)\n",
    "y_asp_true = np.argmax(y_aspect_test, axis=1)\n",
    "\n",
    "print(\"=== Sentiment Classification Report ===\")\n",
    "print(classification_report(y_sent_true, y_sent_pred, target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"=== Aspect Classification Report ===\")\n",
    "print(classification_report(y_asp_true, y_asp_pred, target_names=aspect_encoder.classes_))\n",
    "\n",
    "# Confusion Matrices\n",
    "cm_sent = confusion_matrix(y_sent_true, y_sent_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_sent, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=sentiment_encoder.classes_,\n",
    "            yticklabels=sentiment_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Sentiment Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "cm_asp = confusion_matrix(y_asp_true, y_asp_pred)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_asp, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "            xticklabels=aspect_encoder.classes_,\n",
    "            yticklabels=aspect_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Aspect Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# --- Precision-Recall Curves for Sentiment ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, class_name in enumerate(sentiment_encoder.classes_):\n",
    "    y_true_bin = (y_sent_true == i).astype(int)\n",
    "    y_score = sentiment_probs[:, i]\n",
    "    precision, recall, _ = precision_recall_curve(y_true_bin, y_score)\n",
    "    ap = average_precision_score(y_true_bin, y_score)\n",
    "    plt.plot(recall, precision, label=f\"{class_name} (AP={ap:.2f})\")\n",
    "\n",
    "plt.title(\"Precision-Recall Curve (Sentiment)\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"precision_recall_sentiment.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curves for Aspect ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, class_name in enumerate(aspect_encoder.classes_):\n",
    "    y_true_bin = (y_asp_true == i).astype(int)\n",
    "    y_score = aspect_probs[:, i]\n",
    "    precision, recall, _ = precision_recall_curve(y_true_bin, y_score)\n",
    "    ap = average_precision_score(y_true_bin, y_score)\n",
    "    plt.plot(recall, precision, label=f\"{class_name} (AP={ap:.2f})\")\n",
    "\n",
    "plt.title(\"Precision-Recall Curve (Aspect)\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"precision_recall_aspect.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# --- Calibration Curve for Sentiment ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, class_name in enumerate(sentiment_encoder.classes_):\n",
    "    y_true_bin = (y_sent_true == i).astype(int)\n",
    "    y_score = sentiment_probs[:, i]\n",
    "    prob_true, prob_pred = calibration_curve(y_true_bin, y_score, n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f\"{class_name}\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # perfect calibration line\n",
    "plt.title(\"Calibration Curve (Sentiment)\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Calibration Curve for Aspect ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, class_name in enumerate(aspect_encoder.classes_):\n",
    "    y_true_bin = (y_asp_true == i).astype(int)\n",
    "    y_score = aspect_probs[:, i]\n",
    "    prob_true, prob_pred = calibration_curve(y_true_bin, y_score, n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f\"{class_name}\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title(\"Calibration Curve (Aspect)\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# --- ROC Curves for Sentiment ---\n",
    "# Binarize true labels for multi-class ROC\n",
    "y_sent_true_bin = label_binarize(y_sent_true, classes=range(len(sentiment_encoder.classes_)))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, class_name in enumerate(sentiment_encoder.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_sent_true_bin[:, i], sentiment_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{class_name} (AUC={roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # diagonal line\n",
    "plt.title(\"ROC Curve (Sentiment)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"roc_curve_sentiment.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
