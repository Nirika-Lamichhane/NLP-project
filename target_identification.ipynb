{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772cdf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\OneDrive\\Desktop\\NLP project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def devanagari_lexical_normalization(text):\n",
    "    import re\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. Nukta normalization\n",
    "    # -------------------------\n",
    "    NUKTA_MAP = {\n",
    "        'क़': 'क', 'ख़': 'ख', 'ग़': 'ग', 'ज़': 'ज',\n",
    "        'ड़': 'ड', 'ढ़': 'ढ', 'फ़': 'फ', 'ऱ': 'र', 'य़': 'य'\n",
    "    }\n",
    "    for k, v in NUKTA_MAP.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. Nasal normalization\n",
    "    # -------------------------\n",
    "    NASAL_MAP = {\n",
    "        'ङ्ग': 'ंग','ङ्घ': 'ंघ','ङ्क': 'ंक','ङ्ख': 'ंख',\n",
    "        'ञ्च': 'ंच','ञ्छ': 'ंछ','ञ्ज': 'ंज','ञ्झ': 'ंझ',\n",
    "        'ण्ड': 'ंड','ण्ठ': 'ंठ','ण्ट': 'ंट','ण्ढ': 'ंढ',\n",
    "        'न्द': 'ंद','न्ध': 'ंध','न्त': 'ंत','न्थ': 'ंथ',\n",
    "        'म्प': 'ंप','म्भ': 'ंभ'\n",
    "    }\n",
    "    for k, v in NASAL_MAP.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. Collapse repeated letters\n",
    "    # -------------------------\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4. Remove extra spaces\n",
    "    # -------------------------\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # -------------------------\n",
    "    # 5. Optional entity modifiers\n",
    "    # -------------------------\n",
    "    modifiers = ['जी', 'दाइ', 'चोर', 'भाइ']\n",
    "    for mod in modifiers:\n",
    "        text = re.sub(r'\\b' + mod + r'\\b', '', text)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e6ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 4414\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'dataset/training_dataset.csv'\n",
    "df = pd.read_csv(dataset_path, header=None,\n",
    "                 names=[\"comment\",\"target\",\"aspect\",\"sentiment\"])\n",
    "\n",
    "comments = (\n",
    "    df[\"comment\"]\n",
    "    .astype(str)\n",
    "    .apply(normalize_text)\n",
    "    .apply(devanagari_lexical_normalization)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Total comments:\", len(comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67cc6423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\OneDrive\\Desktop\\NLP project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ACER\\.cache\\huggingface\\hub\\models--Davlan--xlm-roberta-base-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 199/199 [00:01<00:00, 140.63it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mXLMRobertaForTokenClassification LOAD REPORT\u001b[0m from: Davlan/xlm-roberta-base-ner-hrl\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ner_model_name = \"Davlan/xlm-roberta-base-ner-hrl\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75966847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER extraction complete\n",
      "Unique raw entities: 530\n"
     ]
    }
   ],
   "source": [
    "comment_entities = []\n",
    "entity_frequency = defaultdict(int)\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.65\n",
    "\n",
    "for comment in comments:\n",
    "    detected = ner_pipeline(comment)\n",
    "\n",
    "    filtered = []\n",
    "    for ent in detected:\n",
    "        if ent[\"score\"] >= CONFIDENCE_THRESHOLD:\n",
    "            word = normalize_text(ent[\"word\"])\n",
    "            word = devanagari_lexical_normalization(word)\n",
    "\n",
    "            filtered.append({\n",
    "                \"word\": word,\n",
    "                \"label\": ent[\"entity_group\"],\n",
    "                \"score\": float(ent[\"score\"])\n",
    "            })\n",
    "\n",
    "            entity_frequency[word] += 1\n",
    "\n",
    "    comment_entities.append(filtered)\n",
    "\n",
    "print(\"NER extraction complete\")\n",
    "print(\"Unique raw entities:\", len(entity_frequency))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb13eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entities:\n",
      "['सिंहदरबार', 'बालेन', 'सुदन गुरुङ', 'ह', 'र्के', 'ओली', 'देउवा', 'प्रचण्ड', 'बाल', 'रवि नमि', 'हर्क सम्', 'सु', 'दन गुरुङ', 'झा', 'झापा', 'कुकुर', 'के पी ओली', 'केपी शर्मा ओली', 'केपी ओलीको', 'नेपाल']\n"
     ]
    }
   ],
   "source": [
    "unique_entities = list(entity_frequency.keys())\n",
    "\n",
    "print(\"Sample entities:\")\n",
    "print(unique_entities[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "033969a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\OneDrive\\Desktop\\NLP project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ACER\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 263.65it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "entity_embeddings = embedding_model.encode(unique_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c349d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    distance_threshold=0.35,\n",
    "    metric=\"cosine\",\n",
    "    linkage=\"average\"\n",
    ")\n",
    "\n",
    "cluster_ids = clustering.fit_predict(entity_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9deae596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clusters: 71\n"
     ]
    }
   ],
   "source": [
    "cluster_map = defaultdict(list)\n",
    "\n",
    "for entity, cluster_id in zip(unique_entities, cluster_ids):\n",
    "    cluster_map[cluster_id].append(entity)\n",
    "\n",
    "print(\"Total clusters:\", len(cluster_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08624416",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_map = {}\n",
    "\n",
    "for cluster_id, variants in cluster_map.items():\n",
    "    canonical = max(variants, key=lambda v: entity_frequency[v])\n",
    "\n",
    "    for v in variants:\n",
    "        canonical_map[v] = canonical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef95b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_comment_entities = []\n",
    "\n",
    "for ents in comment_entities:\n",
    "    normalized = []\n",
    "\n",
    "    for e in ents:\n",
    "        canonical = canonical_map.get(e[\"word\"], e[\"word\"])\n",
    "\n",
    "        normalized.append({\n",
    "            \"canonical\": canonical,\n",
    "            \"label\": e[\"label\"],\n",
    "            \"score\": e[\"score\"]\n",
    "        })\n",
    "\n",
    "    normalized_comment_entities.append(normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3aafbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL DROPDOWN TARGETS:\n",
      "['आन्तरिक राजस्व विभाग', 'उद्योग', 'एमसीसी', 'एमाले', 'एसजीबी', 'एस्पालि', 'ओली', 'कांग्रेस', 'काङ्ग्रेस', 'किरण', 'कृषि', 'के पी ओली', 'के पी चोर', 'केदार न्यौपाने', 'केवलकार', 'गिरि बन्द', 'चन्द्र सर', 'चाटु मिडिया', 'चिन तिब्बत', 'चिनकाजी महर्जन', 'चीन', 'जिन्दाबाद', 'जु', 'ज्ञान', 'डिल्ली बजार', 'तीन', 'दक्षिण कोरिया', 'दरबारमार्ग कन्सर्ट', 'दिल भुषण पाठक', 'दुर्गा प्रसाई', 'देउबा', 'धादिंग ढोला', 'ध्रुब राठी', 'निर्मला', 'नेपाल', 'नेपाल राष्ट्र बैंकले', 'पशुपतिनाथ', 'पिएचडी', 'बंगलादेश', 'बम', 'बल', 'बहादुर', 'बाल', 'बालेन', 'बि', 'बुढीगण्डकी जलविद्युत आयोजना', 'भारत', 'माओवादी', 'माफ', 'मिटरब्याजी', 'मोदी', 'रमेश लेखक', 'रवि दाई जय घण्टी', 'रवि लामिछाने', 'राजा', 'राम', 'रावण', 'रास्वपा', 'लनाथ बादलको', 'विद्युत प्राधिकरण', 'श्रीलंका', 'सत्य', 'सम्पत्ति शुद्धीकरण', 'सर्वोच्च', 'साउदी', 'सिंहदरबार', 'सुन', 'स्विच बै', 'स्विजरल्याण्ड', 'हरे', 'हर्के माचिक्नी']\n",
      "Total dropdown targets: 71\n"
     ]
    }
   ],
   "source": [
    "dropdown_targets = sorted(list({\n",
    "    e[\"canonical\"]\n",
    "    for comment in normalized_comment_entities\n",
    "    for e in comment\n",
    "}))\n",
    "\n",
    "print(\"\\nFINAL DROPDOWN TARGETS:\")\n",
    "print(dropdown_targets)\n",
    "print(\"Total dropdown targets:\", len(dropdown_targets))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
